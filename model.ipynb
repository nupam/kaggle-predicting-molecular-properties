{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fastai\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from fastai.tabular import *\n",
    "import fastai.text\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "np.range = (lambda x:(x.min(), x.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CudaDeviceProperties(name='GeForce GTX 1060', major=6, minor=1, total_memory=6078MB, multi_processor_count=10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(nn.Module):\n",
    "    def __init__(self, ni, no, usebn=True, act=True, p=0., res=False, bias=True):\n",
    "        \"\"\"\n",
    "        layers in order linear, relu, bn, dropout if residual is false\n",
    "        else linear, relu, drop, add_residual, bn\n",
    "        \"\"\"\n",
    "        super(Dense, self).__init__()\n",
    "        \n",
    "        assert ((not res) or (res and ni==no)), \"input output sizes are different, res connection not possible\"\n",
    "        \n",
    "        self.res    = res\n",
    "        self.usebn  = usebn\n",
    "        \n",
    "        self.layers = [nn.Linear(ni, no, bias= bias)]\n",
    "        if act: self.layers.append(nn.ReLU())\n",
    "        if usebn and not res: self.layers.append(nn.BatchNorm1d(no))\n",
    "        if p > 1e-3: self.layers.append(nn.Dropout(p))\n",
    "            \n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "        if usebn and res:\n",
    "            self.bn = nn.BatchNorm1d(no)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.res: residual = x\n",
    "        for l in self.layers: x = l(x)\n",
    "        if self.res: x += residual\n",
    "        if self.res and self.usebn: x = self.bn(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, d_model=64, p=0.0, n_heads=8):\n",
    "        super(encoder, self).__init__()\n",
    "        self.att = fastai.text.models.MultiHeadAttention(n_heads, d_model, d_head=16, resid_p=p, attn_p=p)\n",
    "        self.ff  = fastai.text.models.feed_forward(d_model, d_model*2, ff_p=p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.att(x)\n",
    "        x = self.ff(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trfmr(nn.Module):\n",
    "    def __init__(self, embed_size=64, inp_len=5, n_enc=5, embed_p=0.3, n_heads=8, p=0.1):\n",
    "        super(trfmr, self).__init__()\n",
    "        self.layers = []\n",
    "        self.layers.append(nn.Embedding(6, embed_size, scale_grad_by_freq=True))\n",
    "        self.layers.append(nn.Dropout(embed_p))\n",
    "        self.layers = nn.ModuleList(self.layers + [encoder(embed_size+inp_len-1, p, n_heads) for _ in range(n_enc)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        atoms = x[:,:, 0].type(torch.LongTensor).cuda() #### note\n",
    "        conts = x[:,:, 1:]\n",
    "        \n",
    "        embeds = self.layers[1](self.layers[0](atoms))\n",
    "        \n",
    "        x = torch.cat((embeds, conts), dim=-1)\n",
    "        \n",
    "        for l in self.layers[2:]:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, embed_size=65, inp_len=16, n_enc=5, embed_p=0.3, n_heads=8, p=0.1, out_p=0.3, inp_meta=16, nmeta=64):\n",
    "        super(model, self).__init__()\n",
    "        self.tf_nfeats= (embed_size+inp_len-1)*5\n",
    "        \n",
    "        self.tf = trfmr(embed_size, inp_len, n_enc, embed_p, n_heads, p)\n",
    "        self.d1 = Dense(self.tf_nfeats, self.tf_nfeats, res=True, p=p)\n",
    "        \n",
    "        self.embed_type_i = nn.Embedding(3, 8, scale_grad_by_freq=True)\n",
    "        self.embed_type_a = nn.Embedding(3, 8, scale_grad_by_freq=True)\n",
    "        self.meta1 = Dense(inp_meta, nmeta, bias=False)\n",
    "        self.meta2 = Dense(nmeta, nmeta, res=True, p=p)\n",
    "        \n",
    "        self.d2 = Dense(self.tf_nfeats+nmeta, self.tf_nfeats+nmeta, res=True, p=p)\n",
    "        self.d3 = Dense(self.tf_nfeats+nmeta, self.tf_nfeats+nmeta, res=True, p=p)\n",
    "        \n",
    "        self.d4 = Dense(self.tf_nfeats+nmeta, 128, p=out_p, bias=False)\n",
    "        self.out = nn.Linear(128, 1, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## x ===> (indices, meta, mol)\n",
    "        \n",
    "        p, q = x[0][:, 0].type(torch.int64), x[0][:, 1].type(torch.int64) ##indices of atom to find scc\n",
    "        meta = x[1] ##input meta features and other\n",
    "        x = x[2] ##input structure\n",
    "        \n",
    "        x = self.tf(x) ##encode\n",
    "        extra = x[:, 0, :]\n",
    "        mask = F.one_hot(p, x.shape[1]).type(torch.bool)\n",
    "        p = x[mask]\n",
    "        mask = F.one_hot(q, x.shape[1]).type(torch.bool)\n",
    "        q = x[mask]\n",
    "        mx = torch.max(x, 1)[0]\n",
    "        mn = torch.mean(x, 1)\n",
    "        \n",
    "        x = torch.cat([extra, p, q, mx, mn], dim=-1)\n",
    "        x = self.d1(x)\n",
    "        \n",
    "        meta = torch.cat((self.embed_type_i(meta[:, 0]), self.embed_type_a(meta[:, 1])), dim=-1)\n",
    "        meta = self.meta1(meta)\n",
    "        meta = self.meta2(meta)\n",
    "        \n",
    "        x = torch.cat([x, meta], dim=-1)\n",
    "        x = self.d2(x)\n",
    "        x = self.d3(x)\n",
    "        x = self.d4(x)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ahn/work/.torch/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: This function is deprecated. Please call randint(0, 3 + 1) instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/media/ahn/work/.torch/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: This function is deprecated. Please call randint(0, 3 + 1) instead\n",
      "  after removing the cwd from sys.path.\n",
      "/media/ahn/work/.torch/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: This function is deprecated. Please call randint(0, 2 + 1) instead\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model(\n",
       "  (tf): trfmr(\n",
       "    (layers): ModuleList(\n",
       "      (0): Embedding(6, 65, scale_grad_by_freq=True)\n",
       "      (1): Dropout(p=0.3)\n",
       "      (2): encoder(\n",
       "        (att): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=80, out_features=384, bias=True)\n",
       "          (out): Linear(in_features=128, out_features=80, bias=True)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([80]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=80, out_features=160, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=160, out_features=80, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([80]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): encoder(\n",
       "        (att): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=80, out_features=384, bias=True)\n",
       "          (out): Linear(in_features=128, out_features=80, bias=True)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([80]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=80, out_features=160, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=160, out_features=80, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([80]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): encoder(\n",
       "        (att): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=80, out_features=384, bias=True)\n",
       "          (out): Linear(in_features=128, out_features=80, bias=True)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([80]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=80, out_features=160, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=160, out_features=80, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([80]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): encoder(\n",
       "        (att): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=80, out_features=384, bias=True)\n",
       "          (out): Linear(in_features=128, out_features=80, bias=True)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([80]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=80, out_features=160, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=160, out_features=80, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([80]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): encoder(\n",
       "        (att): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=80, out_features=384, bias=True)\n",
       "          (out): Linear(in_features=128, out_features=80, bias=True)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([80]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=80, out_features=160, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=160, out_features=80, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([80]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (d1): Dense(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=400, out_features=400, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1)\n",
       "    )\n",
       "    (bn): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (embed_type_i): Embedding(3, 8, scale_grad_by_freq=True)\n",
       "  (embed_type_a): Embedding(3, 8, scale_grad_by_freq=True)\n",
       "  (meta1): Dense(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=16, out_features=64, bias=False)\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (meta2): Dense(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1)\n",
       "    )\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (d2): Dense(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=464, out_features=464, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1)\n",
       "    )\n",
       "    (bn): BatchNorm1d(464, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (d3): Dense(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=464, out_features=464, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1)\n",
       "    )\n",
       "    (bn): BatchNorm1d(464, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (d4): Dense(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=464, out_features=128, bias=False)\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout(p=0.3)\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btt, bs, inp_len= 30, 128, 16\n",
    "inp = np.random.rand(bs, btt, inp_len)\n",
    "inp[:,:, 0] = np.random.random_integers(0, 3, (bs, btt))\n",
    "indices = np.random.random_integers(0, 3, (bs, 2))\n",
    "\n",
    "inp, indices = tensor(inp).type(torch.float32), tensor(indices).type(torch.float32)\n",
    "meta = tensor(np.random.random_integers(0, 2, (bs,2))).type(torch.LongTensor)\n",
    "m = model()\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, inp, meta = indices.cuda(), inp.cuda(), meta.cuda()\n",
    "m = m.to(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3813, device='cuda:0', grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m((indices, meta, inp)).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".torch",
   "language": "python",
   "name": ".torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
